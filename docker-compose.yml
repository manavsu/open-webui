services:
  openwebui:
    image: openwebui
    build:
      context: ./open_webui

      dockerfile: ./Dockerfile
    ports:
      - "8080:8080"
    networks:
      - open-webui_default

  pipelines:
    image: pipelines
    container_name: pipelines
    build:
      context: ./pipelines
      dockerfile: ./Dockerfile
    # environment:
      # - PIPELINES_URLS="https://github.com/open-webui/pipelines/blob/main/examples/pipelines/integrations/python_code_pipeline.py"
    ports:
      - "9099:9099"
    volumes:
      - "./pipelines:/app/pipelines"
    networks:
      - open-webui_default
    depends_on:
       - openwebui


#   ollama:
#     image: ollama/ollama
#     container_name: ollama
#     environment:
#       - GPU=1
#       - NVIDIA_VISIBLE_DEVICES=all
#       - NVIDIA_DRIVER_CAPABILITIES=compute,utility
#     deploy:
#       resources:
#         reservations:
#           devices:
#             - capabilities: [gpu]
#     volumes:
#       - ollama_data:/root/.ollama
#     ports:
#       - "11434:11434"
#     restart: unless-stopped

# volumes:
#   ollama_data:

networks:
  open-webui_default:
    external: true
